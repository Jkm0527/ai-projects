{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install sentence-transformers\nimport os, re, unicodedata, numpy as np, pandas as pd, torch\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import normalize\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\n\nSEED = 42\nnp.random.seed(SEED); torch.manual_seed(SEED)\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n\nTRAIN_PATH = \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"   \nTEST_PATH  = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"    \nMODEL_NAME = \"BAAI/bge-m3\"         # swap to test others\nUSE_E5_PREFIX = False                                 \nBATCH = 256\n\n\ntrain = pd.read_csv(TRAIN_PATH)\nneeded = [\"body\",\"rule\",\"rule_violation\",\n          \"positive_example_1\",\"positive_example_2\",\n          \"negative_example_1\",\"negative_example_2\"]\ntrain = train[[c for c in needed if c in train.columns]].copy()\n\ntest = pd.read_csv(TEST_PATH)\ntest_needed = [\"body\",\"rule\",\"positive_example_1\",\"positive_example_2\",\n               \"negative_example_1\",\"negative_example_2\"]\ntest = test[[c for c in test_needed if c in test.columns]].copy()\nif \"id\" not in test.columns:\n    test[\"id\"] = np.arange(len(test))  \n\n\nUSE_CLEANING = True          \nCLEAN_VARIANT = \"no_stop\"    # one of: \"minimal\",\"norm_only\",\"strip_punct\",\"keep_punct\",\"no_stop\",\"num_mask\"\n\nimport re, unicodedata, string\nURL=r'https?://\\S+'; EMAIL=r'\\S+@\\S+'; USER=r'@\\w+'\n\n\ndef clean_minimal(x: str) -> str:\n    x = unicodedata.normalize(\"NFKC\", x)\n    x = re.sub(URL,\"<URL>\",x)\n    x = re.sub(EMAIL,\"<EMAIL>\",x)\n    x = re.sub(USER,\"<USER>\",x)\n    x = re.sub(r\"\\s+\",\" \",x).strip()\n    return x\n\ndef clean_norm_only(x: str) -> str:\n    x = unicodedata.normalize(\"NFKC\", x)\n    x = re.sub(r\"\\s+\",\" \",x).strip()\n    return x\n\ndef clean_strip_punct(x: str) -> str:\n    x = clean_minimal(x)\n    return x.translate(str.maketrans('', '', string.punctuation))\n\ndef clean_keep_punct(x: str) -> str:\n    x = unicodedata.normalize(\"NFKC\", x)\n    x = re.sub(URL,\"<URL>\",x)\n    x = re.sub(EMAIL,\"<EMAIL>\",x)\n    x = re.sub(USER,\"<USER>\",x)\n    x = re.sub(r\"\\s+\",\" \",x).strip()\n    return x\n\ntry:\n    from nltk.corpus import stopwords\n    _STOP = set(stopwords.words(\"english\"))\nexcept:\n    _STOP = set()\ndef clean_no_stop(x: str) -> str:\n    x = clean_minimal(x).lower()\n    return \" \".join(w for w in x.split() if w not in _STOP) if _STOP else x\n\ndef clean_num_mask(x: str) -> str:\n    x = clean_minimal(x)\n    x = re.sub(r\"\\d+\", \"<NUM>\", x)\n    return x\n\n_VARIANTS = {\n    \"minimal\": clean_minimal,\n    \"norm_only\": clean_norm_only,\n    \"strip_punct\": clean_strip_punct,\n    \"keep_punct\": clean_keep_punct,\n    \"no_stop\": clean_no_stop,     \n    \"num_mask\": clean_num_mask,\n}\n\ndef CLEAN_FN(x):\n    if not isinstance(x, str): \n        x = \"\" if x is None else str(x)\n    if not USE_CLEANING:\n        return x\n    return _VARIANTS.get(CLEAN_VARIANT, clean_minimal)(x)\n\ncols = [c for c in [\"body\",\"rule\",\"positive_example_1\",\"positive_example_2\",\n                    \"negative_example_1\",\"negative_example_2\"] if c in train.columns]\nfor df in (train, test):\n    for c in cols:\n        df[c] = df[c].astype(str).map(CLEAN_FN)\n\ny = train[\"rule_violation\"].astype(int).values\nrules_text = train[\"rule\"].values\n\ndef join2(a,b): return ((a or \"\") + \" \" + (b or \"\")).strip()\ntrain_pos = [join2(a,b) for a,b in zip(train.get(\"positive_example_1\",\"\"), train.get(\"positive_example_2\",\"\"))]\ntrain_neg = [join2(a,b) for a,b in zip(train.get(\"negative_example_1\",\"\"), train.get(\"negative_example_2\",\"\"))]\ntest_pos  = [join2(a,b) for a,b in zip(test.get(\"positive_example_1\",\"\"),  test.get(\"positive_example_2\",\"\"))]\ntest_neg  = [join2(a,b) for a,b in zip(test.get(\"negative_example_1\",\"\"),  test.get(\"negative_example_2\",\"\"))]\n\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nembedder = SentenceTransformer(MODEL_NAME, device=device)\n\nimport gc, torch\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\ndef encode_in_chunks(texts, bs):\n    out = []\n    for i in range(0, len(texts), bs):\n        out.append(embedder.encode(\n            texts[i:i+bs], \n            batch_size=bs, \n            convert_to_numpy=True, \n            show_progress_bar=False\n        ))\n        if torch.cuda.is_available():\n           torch.cuda.empty_cache()\n    return np.vstack(out)\n\n# ---- PROMPT FORMATTING ----\nUSE_PROMPT_FMT = False     # toggle True/False to A/B test\nUSE_E5_PREFIX  = False     # set True for E5 models, keep False for BGE/Qwen\n\ndef fmt_query(body, rule):\n    \"\"\"Format query (comment text + optional rule)\"\"\"\n    if USE_PROMPT_FMT:\n        text = f\"RULE: {rule} || COMMENT: {body}\"\n    else:\n        text = str(body)\n    if USE_E5_PREFIX:\n        text = \"query: \" + text\n    return text\n\ndef fmt_passage(text):\n    \"\"\"Format passage (rules/examples)\"\"\"\n    text = str(text)\n    if USE_E5_PREFIX:\n        text = \"passage: \" + text\n    return text\n\n\n\nq  = [fmt_query(b, r) for b, r in zip(train[\"body\"], train[\"rule\"])]\nr  = [fmt_passage(x)  for x in train[\"rule\"]]\npp = [fmt_passage(a + \" \" + b) for a, b in zip(train.get(\"positive_example_1\",\"\").fillna(\"\"),\n                                               train.get(\"positive_example_2\",\"\").fillna(\"\"))]\nnn = [fmt_passage(a + \" \" + b) for a, b in zip(train.get(\"negative_example_1\",\"\").fillna(\"\"),\n                                               train.get(\"negative_example_2\",\"\").fillna(\"\"))]\n\n\ntq = [fmt_query(b, r) for b, r in zip(test[\"body\"], test[\"rule\"])]\ntr = [fmt_passage(x)  for x in test[\"rule\"]]\ntp = [fmt_passage(a + \" \" + b) for a, b in zip(test.get(\"positive_example_1\",\"\").fillna(\"\"),\n                                               test.get(\"positive_example_2\",\"\").fillna(\"\"))]\ntn = [fmt_passage(a + \" \" + b) for a, b in zip(test.get(\"negative_example_1\",\"\").fillna(\"\"),\n                                               test.get(\"negative_example_2\",\"\").fillna(\"\"))]\n\n\n\nwith torch.no_grad():\n   E_body = encode_in_chunks(q,  BATCH)\nE_rule = encode_in_chunks(r,  BATCH)\nE_ppos = encode_in_chunks(pp, BATCH)\nE_pneg = encode_in_chunks(nn, BATCH)\nTE_body = encode_in_chunks(tq, BATCH)\nTE_rule = encode_in_chunks(tr, BATCH)\nTE_ppos = encode_in_chunks(tp, BATCH)\nTE_pneg = encode_in_chunks(tn, BATCH)\n# L2 normalize\nE_body = normalize(E_body); E_rule = normalize(E_rule)\nE_ppos = normalize(E_ppos); E_pneg = normalize(E_pneg)\nTE_body = normalize(TE_body); TE_rule = normalize(TE_rule)\nTE_ppos = normalize(TE_ppos); TE_pneg = normalize(TE_pneg)\n\ndef tok(s):  \n    return set(s.lower().split())\n\n\ntrain_body_len = train[\"body\"].str.len().to_numpy().reshape(-1,1)\ntrain_rule_len = train[\"rule\"].str.len().to_numpy().reshape(-1,1)\ntest_body_len  = test[\"body\"].str.len().to_numpy().reshape(-1,1)\ntest_rule_len  = test[\"rule\"].str.len().to_numpy().reshape(-1,1)\n\n\ndef jaccard_series(a, b):\n    out = np.zeros((len(a),1), dtype=np.float32)\n    for i,(x,y) in enumerate(zip(a,b)):\n        sx, sy = tok(x), tok(y)\n        u = len(sx|sy); inter = len(sx&sy)\n        out[i,0] = (inter / u) if u else 0.0\n    return out\n\ntrain_jacc = jaccard_series(train[\"body\"], train[\"rule\"])\ntest_jacc  = jaccard_series(test[\"body\"],  test[\"rule\"])\n\ndef zscore_fit_transform(a):\n    m = a.mean(axis=0, keepdims=True); s = a.std(axis=0, keepdims=True) + 1e-9\n    return (a-m)/s, m, s\ndef zscore_transform(a, m, s): return (a-m)/s\n\nnum_train = np.hstack([train_body_len, train_rule_len, train_jacc]).astype(np.float32)\nnum_train_z, m, s = zscore_fit_transform(num_train)\n\nnum_test  = np.hstack([test_body_len, test_rule_len, test_jacc]).astype(np.float32)\nnum_test_z = zscore_transform(num_test, m, s)\n\n\n\nrules_train = train[\"rule\"].astype(str).values\nuniq_rules = np.unique(rules_train)\n\npos_cent = {}\nneg_cent = {}\n\ndef passage_fmt(s):  \n    return (\"passage: \" + s) if USE_E5_PREFIX else s\n\nfor r in uniq_rules:\n    idx = (rules_train == r)\n    pos_txt = (train.loc[idx, \"positive_example_1\"].fillna(\"\") + \" \" +\n               train.loc[idx, \"positive_example_2\"].fillna(\"\")).tolist()\n    neg_txt = (train.loc[idx, \"negative_example_1\"].fillna(\"\") + \" \" +\n               train.loc[idx, \"negative_example_2\"].fillna(\"\")).tolist()\n\n    with torch.no_grad():\n        E_pos = embedder.encode([passage_fmt(x) for x in pos_txt], batch_size=BATCH,\n                                convert_to_numpy=True, show_progress_bar=False)\n        E_neg = embedder.encode([passage_fmt(x) for x in neg_txt], batch_size=BATCH,\n                                convert_to_numpy=True, show_progress_bar=False)\n\n    E_pos = normalize(E_pos); E_neg = normalize(E_neg)\n    pos_cent[r] = E_pos.mean(axis=0, keepdims=True) if len(E_pos) else np.zeros((1, E_body.shape[1]), dtype=np.float32)\n    neg_cent[r] = E_neg.mean(axis=0, keepdims=True) if len(E_neg) else np.zeros((1, E_body.shape[1]), dtype=np.float32)\n\n\nE_posg_train = np.vstack([pos_cent.get(r, np.zeros((1, E_body.shape[1]), np.float32)) for r in rules_train])\nE_negg_train = np.vstack([neg_cent.get(r, np.zeros((1, E_body.shape[1]), np.float32)) for r in rules_train])\n\nrules_test = test[\"rule\"].astype(str).values\nE_posg_test = np.vstack([pos_cent.get(r, np.zeros((1, E_body.shape[1]), np.float32)) for r in rules_test])\nE_negg_test = np.vstack([neg_cent.get(r, np.zeros((1, E_body.shape[1]), np.float32)) for r in rules_test])\n\nif \"subreddit\" in train.columns:\n    sub_train = train[\"subreddit\"].astype(str)\n    sub_test  = test[\"subreddit\"].astype(str)\n    sub_freq  = sub_train.value_counts()\n\n    sub_train_freq = sub_train.map(sub_freq).fillna(1).to_numpy(np.float32).reshape(-1,1)\n    sub_test_freq  = sub_test.map(sub_freq).fillna(1).to_numpy(np.float32).reshape(-1,1)\n\n    TOPK = 30\n    top_subs = list(sub_freq.index[:TOPK])\n    idx = {c:i for i,c in enumerate(top_subs)}\n    sub_train_oh = np.zeros((len(sub_train), len(top_subs)), np.float32)\n    sub_test_oh  = np.zeros((len(sub_test),  len(top_subs)), np.float32)\n    for i, v in enumerate(sub_train):\n        if v in idx: sub_train_oh[i, idx[v]] = 1.0\n    for i, v in enumerate(sub_test):\n        if v in idx: sub_test_oh[i, idx[v]] = 1.0\n\n    m,s = sub_train_freq.mean(0, keepdims=True), sub_train_freq.std(0, keepdims=True)+1e-9\n    sub_train_freq_z = (sub_train_freq - m)/s\n    sub_test_freq_z  = (sub_test_freq  - m)/s\nelse:\n    sub_train_freq_z = np.zeros((len(train),1), np.float32)\n    sub_test_freq_z  = np.zeros((len(test),1),  np.float32)\n    sub_train_oh = np.zeros((len(train),0), np.float32)\n    sub_test_oh  = np.zeros((len(test),0),  np.float32)\n\nrule_freq = pd.Series(rules_train).value_counts()\nr_train_freq = pd.Series(rules_train).map(rule_freq).fillna(1).to_numpy(np.float32).reshape(-1,1)\nr_test_freq  = pd.Series(rules_test ).map(rule_freq).fillna(1).to_numpy(np.float32).reshape(-1,1)\nm,s = r_train_freq.mean(0, keepdims=True), r_train_freq.std(0, keepdims=True)+1e-9\nr_train_freq_z = (r_train_freq - m)/s\nr_test_freq_z  = (r_test_freq  - m)/s\n\nmeta_train = np.hstack([r_train_freq_z, sub_train_freq_z, sub_train_oh]).astype(np.float32)\nmeta_test  = np.hstack([r_test_freq_z,  sub_test_freq_z,  sub_test_oh ]).astype(np.float32)\n\n\ndef cos(A,B): return np.sum(A*B, axis=1, keepdims=True)\n\nX_sim_train = np.hstack([\n    cos(E_body, E_rule),\n    cos(E_body, E_ppos),\n    -cos(E_body, E_pneg),\n    cos(E_body, (E_ppos - E_pneg)),\n])\n\nX_sim_test = np.hstack([\n    cos(TE_body, TE_rule),\n    cos(TE_body, TE_ppos),\n    -cos(TE_body, TE_pneg),\n    cos(TE_body, (TE_ppos - TE_pneg)),\n])\n\nX_sim_train = np.hstack([\n    X_sim_train,\n    cos(E_body, E_posg_train),\n    -cos(E_body, E_negg_train),\n    cos(E_body, (E_posg_train - E_negg_train)),\n])\n\nX_sim_test = np.hstack([\n    X_sim_test,\n    cos(TE_body, E_posg_test),\n    -cos(TE_body, E_negg_test),\n    cos(TE_body, (E_posg_test - E_negg_test)),\n])\n\nX  = np.hstack([X_sim_train, num_train_z]).astype(np.float32)\nXtest = np.hstack([X_sim_test,  num_test_z]).astype(np.float32)\n\nsim_cols = X_sim_train.shape[1]\n\nX_sim_train_z = np.zeros_like(X_sim_train, dtype=np.float32)\nrule_stats = {}\nfor r in uniq_rules:\n    idx = (rules_train == r)\n    mu = X_sim_train[idx].mean(axis=0, keepdims=True)\n    sd = X_sim_train[idx].std(axis=0, keepdims=True) + 1e-9\n    X_sim_train_z[idx] = (X_sim_train[idx] - mu) / sd\n    rule_stats[r] = (mu, sd)\n\nX_sim_test_z = np.zeros_like(X_sim_test, dtype=np.float32)\nfor i, r in enumerate(rules_test):\n    if r in rule_stats:\n        mu, sd = rule_stats[r]\n        X_sim_test_z[i:i+1] = (X_sim_test[i:i+1] - mu) / sd\n    else:\n        X_sim_test_z[i:i+1] = X_sim_test[i:i+1]  \n\nif 'num_train_z' in locals():\n    X     = np.hstack([X_sim_train_z, num_train_z, meta_train]).astype(np.float32)\n    Xtest = np.hstack([X_sim_test_z,  num_test_z,  meta_test ]).astype(np.float32)\nelse:\n    X     = np.hstack([X_sim_train_z,               meta_train]).astype(np.float32)\n    Xtest = np.hstack([X_sim_test_z,                meta_test ]).astype(np.float32)\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\noof = np.zeros(len(X), dtype=np.float32)\n\nrules_all = train[\"rule\"].astype(str).values\nhas_sub   = \"subreddit\" in train.columns\nsubs_all  = train[\"subreddit\"].astype(str).values if has_sub else np.array([\"_NA_\"]*len(train))\n\nfor tr, va in skf.split(X, y):\n    global_rate = y[tr].mean()\n\n    rule_rate = pd.DataFrame({\"r\": rules_all[tr], \"y\": y[tr]}).groupby(\"r\")[\"y\"].mean()\n    te_rule_tr = pd.Series(rules_all[tr]).map(rule_rate).fillna(global_rate).to_numpy(np.float32).reshape(-1,1)\n    te_rule_va = pd.Series(rules_all[va]).map(rule_rate).fillna(global_rate).to_numpy(np.float32).reshape(-1,1)\n\n    if has_sub:\n        sub_rate = pd.DataFrame({\"s\": subs_all[tr], \"y\": y[tr]}).groupby(\"s\")[\"y\"].mean()\n        te_sub_tr = pd.Series(subs_all[tr]).map(sub_rate).fillna(global_rate).to_numpy(np.float32).reshape(-1,1)\n        te_sub_va = pd.Series(subs_all[va]).map(sub_rate).fillna(global_rate).to_numpy(np.float32).reshape(-1,1)\n    else:\n        te_sub_tr = np.zeros((len(tr),1), dtype=np.float32)\n        te_sub_va = np.zeros((len(va),1), dtype=np.float32)\n\n    X_tr = np.hstack([X[tr], te_rule_tr, te_sub_tr]).astype(np.float32)\n    X_va = np.hstack([X[va], te_rule_va, te_sub_va]).astype(np.float32)\n\n    clf = LogisticRegression(max_iter=300, class_weight=\"balanced\")\n    clf.fit(X_tr, y[tr])\n    oof[va] = clf.predict_proba(X_va)[:, 1]\n\nper_rule_auc = {}\nfor rname in np.unique(rules_all):\n    idx = (rules_all == rname)\n    if idx.sum() >= 2 and len(np.unique(y[idx])) > 1:\n        per_rule_auc[rname] = roc_auc_score(y[idx], oof[idx])\n\nmacro_auc = float(np.mean(list(per_rule_auc.values()))) if per_rule_auc else float(\"nan\")\nprint(f\"Column-averaged AUC (CV): {macro_auc:.4f}\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:40:50.515855Z","iopub.execute_input":"2025-09-13T13:40:50.516142Z","iopub.status.idle":"2025-09-13T13:44:38.886622Z","shell.execute_reply.started":"2025-09-13T13:40:50.516119Z","shell.execute_reply":"2025-09-13T13:44:38.885922Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Column-averaged AUC (CV): 0.8413\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# after CV loop produced oof and computed test_probs\nnp.savez(\"/kaggle/working/run_bge_m3_minimal.npz\", # swap with different models after rerunning the finetuning cell\n         oof=oof.astype(np.float32),\n         test=test_probs.astype(np.float32),\n         rules=train[\"rule\"].astype(str).values,\n         y=train[\"rule_violation\"].astype(int).values)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Emsemble\nimport numpy as np, pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\n#  point these to saved runs (oof,y,rules)\nRUN_PATHS = [\n    \"/kaggle/working/run_e5_base.npz\",\n    \"/kaggle/working/run_Alibaba_NLP.npz\",\n    # \"/kaggle/working/run_e5_large.npz\",\n    # \"/kaggle/working/run_bge_m3.npz\",\n]\n\ndef load_run(path):\n    z = np.load(path, allow_pickle=True)\n    return dict(oof=z[\"oof\"].astype(np.float32),\n                y=z[\"y\"].astype(int),\n                rules=z[\"rules\"].astype(str))\n\nruns = [load_run(p) for p in RUN_PATHS]\nassert all((runs[i][\"y\"]==runs[0][\"y\"]).all() for i in range(len(runs))), \"y mismatch\"\nassert all((runs[i][\"rules\"]==runs[0][\"rules\"]).all() for i in range(len(runs))), \"rules mismatch\"\n\ny = runs[0][\"y\"]\nrules = runs[0][\"rules\"]\nO = np.column_stack([r[\"oof\"] for r in runs]) \n\ndef macro_auc_by_rule(y, rules, preds):\n    vals=[]\n    for ru in np.unique(rules):\n        m = (rules == ru)\n        if m.sum() >= 2 and len(np.unique(y[m])) > 1:\n            vals.append(roc_auc_score(y[m], preds[m]))\n    return float(np.mean(vals)) if vals else float(\"nan\")\n \nbest_auc, best_w = -1.0, None\nW_MAX = 4  \nM = O.shape[1]\ndef enum_weights(m, wmax):\n    if m == 1:\n        for w1 in range(1, wmax+1): yield (w1,)\n        return\n    from itertools import product\n    for w in product(range(0, wmax+1), repeat=m):\n        if sum(w) > 0:\n            yield w\n\nfor w in enum_weights(M, W_MAX):\n    blend = (O * np.array(w, dtype=np.float32)).sum(axis=1) / max(1, sum(w))\n    auc = macro_auc_by_rule(y, rules, blend)\n    if auc > best_auc:\n        best_auc, best_w = auc, w\n\nprint(f\"[Grid] Best CV macro AUC: {best_auc:.6f}  weights: {best_w}\")\n\nstack_auc = None\ntry:\n    from sklearn.model_selection import StratifiedKFold\n    skf = StratifiedKFold(5, shuffle=True, random_state=42)\n    oof_stack = np.zeros(len(y), np.float32)\n    for tr, va in skf.split(O, y):\n        clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n        clf.fit(O[tr], y[tr])\n        oof_stack[va] = clf.predict_proba(O[va])[:,1]\n    stack_auc = macro_auc_by_rule(y, rules, oof_stack)\n    print(f\"[Stack] CV macro AUC: {stack_auc:.6f}\")\nexcept Exception as e:\n    print(\"Stacking skipped:\", e)\n\nuse_stacking = (stack_auc is not None) and (stack_auc >= best_auc)\nif use_stacking:\n    print(\">> Using STACKED ensemble.\")\n    oof_final = oof_stack\nelse:\n    print(\">> Using GRID-WEIGHTED average.\")\n    w = np.array(best_w, np.float32)\n    oof_final = (O * w).sum(axis=1) / max(1, w.sum())\n\nfinal_auc = macro_auc_by_rule(y, rules, oof_final)\nprint(f\"Final chosen CV macro AUC: {final_auc:.6f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}